{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2L91fr4uGio"
      },
      "source": [
        "# Assignment 3: Algorithms for text generation\n",
        "\n",
        "In this assignment, we will explore using trained language models to generate text. In particular, we will work with a recent model called [Generative Pre-trained Transformer, version 2 \\(GPT-2\\)](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) that was published in 2019 by OpenAI.\n",
        "\n",
        "As language models are probabilistic models of text, there are different methods of generating (also known as _decoding_) text strings from the model, as we have seen in [one of the lectures](http://www.cse.chalmers.se/~richajo/dat450/lectures/l7/m7_3.pdf). You will implement some of the most common decoding methods in this assignment, and later reflect on the qualitative aspects of the different methods.\n",
        "\n",
        "**Note:** It will be important to use a GPU with a large memory, such as provided on Colab. Please enable the GPU runtime by going to _Runtime -> Change Runtime type -> GPU_.\n",
        "\n",
        "**Note:** Implementations of the generation algorithms you code here already exist in the Huggingface library. In a real use case, you would typically just call `generate`. These reimplementations are for pedagogical purposes.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJT7isgJRZ_X",
        "outputId": "14e32ddb-a595-4904-a54d-968fdf477929"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7ae9d5218a60>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's start by importing the PyTorch library:\n",
        "\n",
        "import torch\n",
        "torch.set_grad_enabled(False) # since we will not be updating any models...\n",
        "# Context-manager that sets gradient calculation on or of"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pInbcRpvODQP"
      },
      "source": [
        "## The GPT-2 Language Model\n",
        "\n",
        "In the GPT-2, a _Transformer Decoder_ is used to model the conditional probability $P(x_i | x_1, ..., x_{i-1})$ using large quantities of text data. As training big language models are typically very computationally expensive, we will not train our own in this assignment, but use a pre-trained one instead. For this we will need to install a separate package, called `transformers`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0igih6ZS9Al",
        "outputId": "d0a637f6-3245-46c4-f3e2-8c157394271e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWonc2QlTaWL"
      },
      "source": [
        "The GPT-2 model comes with its own tokenizer, which we will need to load:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z274C44T5QDw"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144,
          "referenced_widgets": [
            "c652396a83794ac69546f30d2835465b",
            "2a45eb137d744daf86b8cddecb1fa45b",
            "df767a8f835743ceacaa0e12f97764ee",
            "f302491f40944da8b523a16568ccef62",
            "109e5dca8c534b83a6a105eac4513a15",
            "3d132be2a19a4d9cb509efb10dd3f9fe",
            "9e36706c5a414fb99bf84530c24c8833",
            "da0c46b2095648719920b0ea2a370adb",
            "d31c233d5d4040ffb737f7430de0a9f4",
            "7d756075682f47d19d1bd770ead65dfc",
            "6ed9b3d23ec04503b21dc9fcb77dd22d",
            "fe77632b79c74c76a9fa7c06d3d4319c",
            "36d3427b58d24f9dadb87a5bb9eda50c",
            "a3738530b7cd45a6b942d527d2ab44fb",
            "fc2d8b71ba5f4a70b80786cf3f8af777",
            "ec0dc03e55bb496c9d00afd68a830222",
            "b318ffe4b74c4da49364ab3f379ac346",
            "38c87faad95e436e964a8071ab4685c1",
            "d6ee296100134d13a7e1add3cc7c211c",
            "40b4b787d7f6446abc2f1a4635f3941f",
            "c387efc033b44482906e515805b96ab2",
            "36fee36832fe4d559d4aecff64d1a7cb",
            "b51cb9d84f5347ab837c18e3998e6e7c",
            "3ecb216b315e40b78d26545a8ea9fcd3",
            "c5d6b40b40834ebdb3f0d8b22a13c521",
            "da63eeec4efe4616a7af9150c97bc445",
            "c7d54c25e0a94b419bcf6d0cfc168aa2",
            "314bc76d95cb49b1ba9c855c4e31083e",
            "63e37183b78b425e8cae171a6358f4b2",
            "e125b0e8b8fb495b8d2e71680cc4e2db",
            "6b44f27cffcd4d3fa10f4199a59a1af3",
            "e70c90f04dc146209d7bec6caaf1ea67",
            "a0c5eda64b7246f3a56794241b96776d",
            "90204f83da3d470db0e6a094d8704aaa",
            "8b849a2c588d4b2b869e73c810cf558e",
            "6e5b60bc3e3c416e925e37101a27c92a",
            "e14150ca8c984aeca35e28d95b542f47",
            "789acacc5b4d4dc883fe202cefb3131b",
            "1a962fc4f371442f8797bb5c4dd10b3e",
            "07a467f91add48c28fbf96f5753d4eb7",
            "7373f62232f94497be706d60c3cbfc7a",
            "97922be0978045a39018ae863f42a150",
            "d6cc1c0e7a3a4abdb6d0fc138f8dee94",
            "ccb1ff93fc034e0898050b50c5faf8ad"
          ]
        },
        "id": "xgYlfHZRAprm",
        "outputId": "992879de-f0e4-425f-e26a-4ad050048ea2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c652396a83794ac69546f30d2835465b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe77632b79c74c76a9fa7c06d3d4319c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b51cb9d84f5347ab837c18e3998e6e7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90204f83da3d470db0e6a094d8704aaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# from_pretrained method is used to load a pre-trained tokenizer for the specified model.\n",
        "# In this case, it loads the GPT-2 tokenizer for the \"gpt2-large\" variant,\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
        "#  sets the padding token of the tokenizer to be the end-of-sequence (eos) token.\n",
        "# In GPT-2, the end-of-sequence token is often used to mark the end of a sequence of text.\n",
        "# By setting the padding token to be the eos token,  it indicates that when padding is applied to sequences,\n",
        "# the eos token will be used as the padding token.\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6nPe90iUaW6",
        "outputId": "0da2f4b7-e62c-4419-ed16-2de074a089a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[   45, 19930,  6296,   329,  3288]], device='cuda:0')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids = tokenizer.encode(\"NLP stands for natural\", return_tensors=\"pt\").to(\"cuda\")\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qau4qvB2Wiyz"
      },
      "source": [
        "Like the tokenizers we've seen earlier in the course, it maps a text string to a sequence of tokens (integers) from a fixed size vocabulary. Note that `input_ids` is two-dimensional, where the first dimension is the batch dimension, and second dimension is the sequence dimension.\n",
        "The tokenizer can also decode the integers back to the string representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "1BHVbWLa9ssv",
        "outputId": "a89c9cf3-bc9a-4ed7-fa4d-71c5b5111622"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'NLP stands for natural'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Msvqob9j5A"
      },
      "source": [
        "We can now download the trained model. As we will work with a large model in this assignment (several hundreds of millions of parameters), using a GPU will _greatly_ speed up predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "66c73a0a40eb4d21a8e7cea46d5511bc",
            "c1d44aa06b4b425b9b855bc45b384c7c",
            "f536628934f84bfaabcd9ec534f46c37",
            "10c9a2dd1a6f48fdb8556f87dbdf6b1f",
            "a17b4b04bccf4df2b8931b122fdffcbe",
            "2375f9d52a074b21a4ef3070f93ebc82",
            "33c3b60620534f5b958bcea306dfb179",
            "192b02a69f0d4b26b74a5deee1c7a072",
            "39d8fa957d864bea82a3155333956e4b",
            "36815735052f49df849aa69e583fcb55",
            "9bed20f2e9bb4c53b602e6d3d70bf81f",
            "1ab070c6ef9b4f7c9ddf830320934090",
            "d56d850a9cca4d91b8532c7d4d9b7c96",
            "cad9fdc7f67c4ab186f1205200b1a225",
            "94270c7ecd294e6a84cfcbd49a41e7e4",
            "a6fb1d2175a34d609eb59f9177757fc9",
            "2a751c76b602418b9776483451645a3f",
            "f7656d77ee13492c871492c368fa3eef",
            "6cff7ba1a249473a8ecd4d4a5ab62731",
            "50e447645d784968a5f2b1d4c81cbe09",
            "76046035df4e4587aadeb5ed3fd62c5a",
            "040bedf3b53749328230744333f534fc"
          ]
        },
        "id": "Cuyqt4X9XBKB",
        "outputId": "3d156cd6-7c14-4db8-9c6b-bba98b634f83"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66c73a0a40eb4d21a8e7cea46d5511bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ab070c6ef9b4f7c9ddf830320934090",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\").to(\"cuda\").eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJytCxvYTLWS"
      },
      "source": [
        "For your curiosity, you can optionally print the number of parameters in each layer and the total number of parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeXUeGlnXHuz",
        "outputId": "c0d3a516-ceb0-441b-ceea-6c94d9bca994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformer.wte.weight: 64328960 parameters\n",
            "transformer.wpe.weight: 1310720 parameters\n",
            "transformer.h.0.ln_1.weight: 1280 parameters\n",
            "transformer.h.0.ln_1.bias: 1280 parameters\n",
            "transformer.h.0.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.0.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.0.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.0.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.0.ln_2.weight: 1280 parameters\n",
            "transformer.h.0.ln_2.bias: 1280 parameters\n",
            "transformer.h.0.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.0.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.0.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.0.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.1.ln_1.weight: 1280 parameters\n",
            "transformer.h.1.ln_1.bias: 1280 parameters\n",
            "transformer.h.1.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.1.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.1.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.1.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.1.ln_2.weight: 1280 parameters\n",
            "transformer.h.1.ln_2.bias: 1280 parameters\n",
            "transformer.h.1.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.1.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.1.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.1.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.2.ln_1.weight: 1280 parameters\n",
            "transformer.h.2.ln_1.bias: 1280 parameters\n",
            "transformer.h.2.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.2.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.2.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.2.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.2.ln_2.weight: 1280 parameters\n",
            "transformer.h.2.ln_2.bias: 1280 parameters\n",
            "transformer.h.2.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.2.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.2.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.2.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.3.ln_1.weight: 1280 parameters\n",
            "transformer.h.3.ln_1.bias: 1280 parameters\n",
            "transformer.h.3.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.3.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.3.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.3.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.3.ln_2.weight: 1280 parameters\n",
            "transformer.h.3.ln_2.bias: 1280 parameters\n",
            "transformer.h.3.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.3.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.3.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.3.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.4.ln_1.weight: 1280 parameters\n",
            "transformer.h.4.ln_1.bias: 1280 parameters\n",
            "transformer.h.4.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.4.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.4.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.4.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.4.ln_2.weight: 1280 parameters\n",
            "transformer.h.4.ln_2.bias: 1280 parameters\n",
            "transformer.h.4.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.4.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.4.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.4.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.5.ln_1.weight: 1280 parameters\n",
            "transformer.h.5.ln_1.bias: 1280 parameters\n",
            "transformer.h.5.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.5.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.5.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.5.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.5.ln_2.weight: 1280 parameters\n",
            "transformer.h.5.ln_2.bias: 1280 parameters\n",
            "transformer.h.5.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.5.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.5.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.5.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.6.ln_1.weight: 1280 parameters\n",
            "transformer.h.6.ln_1.bias: 1280 parameters\n",
            "transformer.h.6.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.6.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.6.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.6.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.6.ln_2.weight: 1280 parameters\n",
            "transformer.h.6.ln_2.bias: 1280 parameters\n",
            "transformer.h.6.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.6.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.6.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.6.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.7.ln_1.weight: 1280 parameters\n",
            "transformer.h.7.ln_1.bias: 1280 parameters\n",
            "transformer.h.7.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.7.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.7.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.7.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.7.ln_2.weight: 1280 parameters\n",
            "transformer.h.7.ln_2.bias: 1280 parameters\n",
            "transformer.h.7.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.7.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.7.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.7.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.8.ln_1.weight: 1280 parameters\n",
            "transformer.h.8.ln_1.bias: 1280 parameters\n",
            "transformer.h.8.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.8.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.8.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.8.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.8.ln_2.weight: 1280 parameters\n",
            "transformer.h.8.ln_2.bias: 1280 parameters\n",
            "transformer.h.8.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.8.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.8.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.8.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.9.ln_1.weight: 1280 parameters\n",
            "transformer.h.9.ln_1.bias: 1280 parameters\n",
            "transformer.h.9.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.9.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.9.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.9.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.9.ln_2.weight: 1280 parameters\n",
            "transformer.h.9.ln_2.bias: 1280 parameters\n",
            "transformer.h.9.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.9.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.9.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.9.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.10.ln_1.weight: 1280 parameters\n",
            "transformer.h.10.ln_1.bias: 1280 parameters\n",
            "transformer.h.10.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.10.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.10.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.10.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.10.ln_2.weight: 1280 parameters\n",
            "transformer.h.10.ln_2.bias: 1280 parameters\n",
            "transformer.h.10.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.10.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.10.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.10.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.11.ln_1.weight: 1280 parameters\n",
            "transformer.h.11.ln_1.bias: 1280 parameters\n",
            "transformer.h.11.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.11.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.11.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.11.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.11.ln_2.weight: 1280 parameters\n",
            "transformer.h.11.ln_2.bias: 1280 parameters\n",
            "transformer.h.11.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.11.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.11.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.11.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.12.ln_1.weight: 1280 parameters\n",
            "transformer.h.12.ln_1.bias: 1280 parameters\n",
            "transformer.h.12.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.12.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.12.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.12.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.12.ln_2.weight: 1280 parameters\n",
            "transformer.h.12.ln_2.bias: 1280 parameters\n",
            "transformer.h.12.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.12.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.12.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.12.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.13.ln_1.weight: 1280 parameters\n",
            "transformer.h.13.ln_1.bias: 1280 parameters\n",
            "transformer.h.13.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.13.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.13.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.13.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.13.ln_2.weight: 1280 parameters\n",
            "transformer.h.13.ln_2.bias: 1280 parameters\n",
            "transformer.h.13.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.13.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.13.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.13.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.14.ln_1.weight: 1280 parameters\n",
            "transformer.h.14.ln_1.bias: 1280 parameters\n",
            "transformer.h.14.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.14.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.14.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.14.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.14.ln_2.weight: 1280 parameters\n",
            "transformer.h.14.ln_2.bias: 1280 parameters\n",
            "transformer.h.14.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.14.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.14.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.14.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.15.ln_1.weight: 1280 parameters\n",
            "transformer.h.15.ln_1.bias: 1280 parameters\n",
            "transformer.h.15.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.15.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.15.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.15.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.15.ln_2.weight: 1280 parameters\n",
            "transformer.h.15.ln_2.bias: 1280 parameters\n",
            "transformer.h.15.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.15.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.15.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.15.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.16.ln_1.weight: 1280 parameters\n",
            "transformer.h.16.ln_1.bias: 1280 parameters\n",
            "transformer.h.16.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.16.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.16.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.16.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.16.ln_2.weight: 1280 parameters\n",
            "transformer.h.16.ln_2.bias: 1280 parameters\n",
            "transformer.h.16.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.16.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.16.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.16.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.17.ln_1.weight: 1280 parameters\n",
            "transformer.h.17.ln_1.bias: 1280 parameters\n",
            "transformer.h.17.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.17.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.17.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.17.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.17.ln_2.weight: 1280 parameters\n",
            "transformer.h.17.ln_2.bias: 1280 parameters\n",
            "transformer.h.17.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.17.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.17.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.17.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.18.ln_1.weight: 1280 parameters\n",
            "transformer.h.18.ln_1.bias: 1280 parameters\n",
            "transformer.h.18.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.18.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.18.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.18.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.18.ln_2.weight: 1280 parameters\n",
            "transformer.h.18.ln_2.bias: 1280 parameters\n",
            "transformer.h.18.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.18.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.18.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.18.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.19.ln_1.weight: 1280 parameters\n",
            "transformer.h.19.ln_1.bias: 1280 parameters\n",
            "transformer.h.19.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.19.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.19.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.19.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.19.ln_2.weight: 1280 parameters\n",
            "transformer.h.19.ln_2.bias: 1280 parameters\n",
            "transformer.h.19.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.19.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.19.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.19.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.20.ln_1.weight: 1280 parameters\n",
            "transformer.h.20.ln_1.bias: 1280 parameters\n",
            "transformer.h.20.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.20.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.20.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.20.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.20.ln_2.weight: 1280 parameters\n",
            "transformer.h.20.ln_2.bias: 1280 parameters\n",
            "transformer.h.20.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.20.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.20.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.20.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.21.ln_1.weight: 1280 parameters\n",
            "transformer.h.21.ln_1.bias: 1280 parameters\n",
            "transformer.h.21.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.21.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.21.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.21.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.21.ln_2.weight: 1280 parameters\n",
            "transformer.h.21.ln_2.bias: 1280 parameters\n",
            "transformer.h.21.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.21.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.21.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.21.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.22.ln_1.weight: 1280 parameters\n",
            "transformer.h.22.ln_1.bias: 1280 parameters\n",
            "transformer.h.22.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.22.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.22.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.22.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.22.ln_2.weight: 1280 parameters\n",
            "transformer.h.22.ln_2.bias: 1280 parameters\n",
            "transformer.h.22.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.22.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.22.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.22.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.23.ln_1.weight: 1280 parameters\n",
            "transformer.h.23.ln_1.bias: 1280 parameters\n",
            "transformer.h.23.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.23.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.23.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.23.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.23.ln_2.weight: 1280 parameters\n",
            "transformer.h.23.ln_2.bias: 1280 parameters\n",
            "transformer.h.23.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.23.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.23.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.23.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.24.ln_1.weight: 1280 parameters\n",
            "transformer.h.24.ln_1.bias: 1280 parameters\n",
            "transformer.h.24.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.24.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.24.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.24.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.24.ln_2.weight: 1280 parameters\n",
            "transformer.h.24.ln_2.bias: 1280 parameters\n",
            "transformer.h.24.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.24.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.24.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.24.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.25.ln_1.weight: 1280 parameters\n",
            "transformer.h.25.ln_1.bias: 1280 parameters\n",
            "transformer.h.25.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.25.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.25.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.25.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.25.ln_2.weight: 1280 parameters\n",
            "transformer.h.25.ln_2.bias: 1280 parameters\n",
            "transformer.h.25.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.25.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.25.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.25.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.26.ln_1.weight: 1280 parameters\n",
            "transformer.h.26.ln_1.bias: 1280 parameters\n",
            "transformer.h.26.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.26.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.26.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.26.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.26.ln_2.weight: 1280 parameters\n",
            "transformer.h.26.ln_2.bias: 1280 parameters\n",
            "transformer.h.26.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.26.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.26.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.26.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.27.ln_1.weight: 1280 parameters\n",
            "transformer.h.27.ln_1.bias: 1280 parameters\n",
            "transformer.h.27.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.27.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.27.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.27.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.27.ln_2.weight: 1280 parameters\n",
            "transformer.h.27.ln_2.bias: 1280 parameters\n",
            "transformer.h.27.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.27.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.27.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.27.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.28.ln_1.weight: 1280 parameters\n",
            "transformer.h.28.ln_1.bias: 1280 parameters\n",
            "transformer.h.28.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.28.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.28.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.28.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.28.ln_2.weight: 1280 parameters\n",
            "transformer.h.28.ln_2.bias: 1280 parameters\n",
            "transformer.h.28.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.28.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.28.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.28.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.29.ln_1.weight: 1280 parameters\n",
            "transformer.h.29.ln_1.bias: 1280 parameters\n",
            "transformer.h.29.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.29.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.29.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.29.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.29.ln_2.weight: 1280 parameters\n",
            "transformer.h.29.ln_2.bias: 1280 parameters\n",
            "transformer.h.29.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.29.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.29.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.29.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.30.ln_1.weight: 1280 parameters\n",
            "transformer.h.30.ln_1.bias: 1280 parameters\n",
            "transformer.h.30.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.30.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.30.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.30.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.30.ln_2.weight: 1280 parameters\n",
            "transformer.h.30.ln_2.bias: 1280 parameters\n",
            "transformer.h.30.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.30.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.30.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.30.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.31.ln_1.weight: 1280 parameters\n",
            "transformer.h.31.ln_1.bias: 1280 parameters\n",
            "transformer.h.31.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.31.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.31.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.31.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.31.ln_2.weight: 1280 parameters\n",
            "transformer.h.31.ln_2.bias: 1280 parameters\n",
            "transformer.h.31.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.31.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.31.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.31.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.32.ln_1.weight: 1280 parameters\n",
            "transformer.h.32.ln_1.bias: 1280 parameters\n",
            "transformer.h.32.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.32.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.32.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.32.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.32.ln_2.weight: 1280 parameters\n",
            "transformer.h.32.ln_2.bias: 1280 parameters\n",
            "transformer.h.32.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.32.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.32.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.32.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.33.ln_1.weight: 1280 parameters\n",
            "transformer.h.33.ln_1.bias: 1280 parameters\n",
            "transformer.h.33.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.33.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.33.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.33.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.33.ln_2.weight: 1280 parameters\n",
            "transformer.h.33.ln_2.bias: 1280 parameters\n",
            "transformer.h.33.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.33.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.33.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.33.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.34.ln_1.weight: 1280 parameters\n",
            "transformer.h.34.ln_1.bias: 1280 parameters\n",
            "transformer.h.34.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.34.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.34.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.34.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.34.ln_2.weight: 1280 parameters\n",
            "transformer.h.34.ln_2.bias: 1280 parameters\n",
            "transformer.h.34.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.34.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.34.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.34.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.h.35.ln_1.weight: 1280 parameters\n",
            "transformer.h.35.ln_1.bias: 1280 parameters\n",
            "transformer.h.35.attn.c_attn.weight: 4915200 parameters\n",
            "transformer.h.35.attn.c_attn.bias: 3840 parameters\n",
            "transformer.h.35.attn.c_proj.weight: 1638400 parameters\n",
            "transformer.h.35.attn.c_proj.bias: 1280 parameters\n",
            "transformer.h.35.ln_2.weight: 1280 parameters\n",
            "transformer.h.35.ln_2.bias: 1280 parameters\n",
            "transformer.h.35.mlp.c_fc.weight: 6553600 parameters\n",
            "transformer.h.35.mlp.c_fc.bias: 5120 parameters\n",
            "transformer.h.35.mlp.c_proj.weight: 6553600 parameters\n",
            "transformer.h.35.mlp.c_proj.bias: 1280 parameters\n",
            "transformer.ln_f.weight: 1280 parameters\n",
            "transformer.ln_f.bias: 1280 parameters\n",
            "Total number of parameters: 774030080\n"
          ]
        }
      ],
      "source": [
        "total_parameters = 0\n",
        "for name, par in model.named_parameters():\n",
        "  n_par = 1\n",
        "  for d in par.shape:\n",
        "    n_par *= d\n",
        "  print(f'{name}: {n_par} parameters')\n",
        "  total_parameters += n_par\n",
        "print(f'Total number of parameters: {total_parameters}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3IcB6Rf1-7Z"
      },
      "source": [
        "With the model loaded, let's use it for predicting the next token of our tokenized `input_ids` from above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRsY9J1214od",
        "outputId": "00e33180-37c5-4191-f385-1efe84705cab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.7978,  2.8982,  1.6116,  ..., -5.3930, -5.0412,  0.7757],\n",
              "         [ 2.0065,  5.6440,  0.4546,  ..., -3.7273, -6.4793,  1.1439],\n",
              "         [ 2.4830,  2.4556, -1.5678,  ..., -6.9141, -6.2589,  1.0836],\n",
              "         [ 1.6584,  4.0486, -3.9575,  ..., -6.5006, -7.1734,  1.3958],\n",
              "         [ 2.0553,  3.9053, -1.5021,  ..., -2.4546, -7.9215,  1.0312]]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = model(input_ids=input_ids)\n",
        "\n",
        "predictions.logits\n",
        "# predictions.logits typically refers to the raw unnormalized scores produced by the model for each token in the vocabulary.\n",
        "# It's essentially the output of the model's last layer before applying a softmax activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6d0sDZZ2iQ-",
        "outputId": "ad5751f7-a7e9-49fc-9898-a984d199607a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 50257])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions.logits.shape\n",
        "# the size should be: batch_size, sequence_length, vocabulary_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhXpfjNG2_R-"
      },
      "source": [
        "What we get from the model are the unnormalized log probabilities, called _logits_.\n",
        "\n",
        "**Self-check:** Look at the shape of `predictions.logits` from above, what do the three dimensions represent?\n",
        "\n",
        "**Your work:** How can we, from `predictions.logits`, compute the actual probability distribution of the next word in the sequence `NLP stands for natural ____`? The distribution should be over the entire vocabulary, and be valid probabilities that sum to one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zYkIDnA2-pB",
        "outputId": "d8bdd168-e52e-43ff-aad2-66ee62b4baba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([50257])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next_token_prob = torch.nn.functional.softmax(predictions.logits[0,-1], dim = 0)# WRITE CODE HERE\n",
        "next_token_prob.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OQpZiarTZLb2"
      },
      "outputs": [],
      "source": [
        "# These tests should pass without modifications\n",
        "assert next_token_prob.shape == torch.Size([tokenizer.vocab_size])\n",
        "assert abs(next_token_prob.sum() - 1.0) < 0.01\n",
        "assert all(next_token_prob >= 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhlGz05G7phK"
      },
      "source": [
        "**Your work:** Compute the top 5 most probable next tokens, based on the `next_token_prob` distribution.\n",
        "\n",
        "**Hint**: the function [`topk`](https://pytorch.org/docs/stable/generated/torch.topk.html) will be useful here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jWTNJo2Yatts"
      },
      "outputs": [],
      "source": [
        "top_5_next_tokens = torch.topk(next_token_prob, 5).indices# WRITE CODE HERE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfoIKaHkT5-R"
      },
      "source": [
        "We can again use `tokenizer.decode` to map the integer-encoded tokens back to strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9XYdwTkAR3r",
        "outputId": "709102a3-ce4c-43a8-db0f-ce72e422866c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " language\n",
            " Language\n",
            " human\n",
            " languages\n",
            "-\n"
          ]
        }
      ],
      "source": [
        "for index in top_5_next_tokens:\n",
        "  print(f\"{tokenizer.decode([index])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwpfDMWv_KpA"
      },
      "source": [
        "We could now decide to, for example, pick the token id with the highest probability, append that to our input, and run through the model again to compute the distribution for the next token again.\n",
        "\n",
        "**Your work:** Take the higest predicted token from `top_5_next_tokens` and append to `input_ids`\n",
        "\n",
        "**Hint:** The function [`torch.cat`](https://pytorch.org/docs/stable/generated/torch.cat.html) could be useful here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Uv2j0_fTC0bB"
      },
      "outputs": [],
      "source": [
        "# torch.cat(tensors, dim=0, *, out=None) → Tensor\n",
        "# Concatenates the given sequence of seq tensors in the given dimension.\n",
        "# All tensors must either have the same shape (except in the concatenating dimension) or be empty.\n",
        "new_input_ids = torch.cat([input_ids, torch.tensor([[top_5_next_tokens[0]]], device=input_ids.device)], dim=1)# WRITE CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0KWVpQWNMfh"
      },
      "source": [
        "To see that the prediction is sensible, you can convert the integer-encoded tensor back into text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "FLU2ykBYHTlB",
        "outputId": "d42bce7a-1481-4bb6-e7f9-9bd280b31dbf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'NLP stands for natural language'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(new_input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GYQc7jBIEmTZ"
      },
      "outputs": [],
      "source": [
        "# These tests should pass without modifications\n",
        "assert new_input_ids.shape == torch.Size([1, input_ids.shape[1] + 1])\n",
        "assert new_input_ids[0, -1] == top_5_next_tokens[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzhmkbRoFuQo"
      },
      "source": [
        "**Your work:** Like above, compute a new distribution for the next token and print the top 5 most probable next tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo6-TiumGR3_",
        "outputId": "85189083-4c83-4f3e-c0e8-e78e05cb8786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " processing\n",
            " understanding\n",
            " perception\n",
            " parsing\n",
            " generation\n"
          ]
        }
      ],
      "source": [
        "# WRITE CODE HERE\n",
        "predictions = model(input_ids=new_input_ids)\n",
        "next_token_prob = torch.nn.functional.softmax(predictions.logits[0,-1], dim = 0)\n",
        "top_5_next_tokens = torch.topk(next_token_prob, 5).indices\n",
        "for index in top_5_next_tokens:\n",
        "  print(f\"{tokenizer.decode([index])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2s2Noe2-8JO"
      },
      "source": [
        "## Generating from a language model\n",
        "\n",
        "What we just did can be formalized into a general algorithm to generate text from a language model:\n",
        "\n",
        "1. Start with some text to be _continued_. We will denote this initial text as a _prompt_: $x_1, ..., x_i$\n",
        "2. Use the language model to compute the next token probabilities: $P(x_{i+1} | x_1, ..., x_i)$\n",
        "3. Based on the distribution, pick some next token $x_{i+1}$ and append to the input\n",
        "4. Repeat from step 2 until a stopping criterion is met."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOxNKTKI-eLd"
      },
      "source": [
        "An important decision when generating from language models is what strategy you apply for picking next tokens (step 3). We will implement and experiment with different such strategies and you will in the individual reflection discuss pros and cons of each, and how these differ from each other.\n",
        "\n",
        "We begin by defining an abstract decoding strategy class, that has a method `step` which takes the `logits` and `input_ids` at some step. `step(...)` returns updated `input_ids` to be used in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TNa0vxutLxqH"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class DecodingStrategy(ABC):\n",
        "\n",
        "  @abstractmethod\n",
        "  def step(self, logits, input_ids):\n",
        "    \"\"\"\n",
        "    This method takes next token logits and input_ids and applies some strategy to update the input_ids.\n",
        "    It returns the updated input ids.\n",
        "\n",
        "    Args:\n",
        "      logits:    3d float tensor\n",
        "      input_ids: 2d int tensor\n",
        "\n",
        "    Returns:\n",
        "      next_input_ids: 2d int tensor\n",
        "    \"\"\"\n",
        "    next_token_prob = torch.nn.functional.softmax(predictions.logits[0,-1], dim = 0)\n",
        "    next_token_index = torch.topk(next_token_prob, 1).indices\n",
        "    new_input_ids = torch.cat([input_ids, torch.tensor([[next_token_index]], device=input_ids.device)], dim=1)\n",
        "\n",
        "    return input_ids\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuhKGbmRu3Uv"
      },
      "source": [
        "Next, we will implement a stopping criterion. In this assignment we will stop when the model has generated X number of sentences. We define sentence boundaries by the period token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUoLnLUbvz4q",
        "outputId": "7da65289-5caf-484e-d42c-2f225f06c17c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[13]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode(\".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOOTkIx4v6QZ"
      },
      "source": [
        "**Your work:** Implement the following function, that returns the number of completed sentences in each batch sequence in `input_ids`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "W1v4oySvvLBQ"
      },
      "outputs": [],
      "source": [
        "test_input_ids = tokenizer([\"This sequence has zero completed sentences\", \"Here is one completed sentence. Here is another.\"], return_tensors=\"pt\", padding=True).input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "riYe7Uu2wKrc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_num_sentences(input_ids):\n",
        "  \"\"\"\n",
        "  Returns an integer tensor of shape input_ids.shape[0], that tells how many completed\n",
        "  sentences there are in each batch sequence\n",
        "  \"\"\"\n",
        "\n",
        "  # WRITE CODE HERE\n",
        "  batch_size = input_ids.shape[0]\n",
        "  count = np.zeros(batch_size, dtype = int)\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    for words in input_ids[i]:\n",
        "      if words == torch.tensor(tokenizer.encode(\".\"), device=input_ids.device):\n",
        "        count[i] +=1\n",
        "\n",
        "  return torch.tensor(count)\n",
        "\n",
        "\n",
        "  raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fZRo-yc0wmre"
      },
      "outputs": [],
      "source": [
        "# This test should pass without modification\n",
        "test_input_ids = tokenizer([\"This sequence has zero completed sentences\", \"Here is one completed sentence. Here is another.\"], return_tensors=\"pt\", padding=True).input_ids\n",
        "assert torch.equal(get_num_sentences(test_input_ids), torch.tensor([0, 2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtokkoSzzYp7"
      },
      "source": [
        "**Your work:** Implement the stopping criterion function below, that returns a boolean vector indicating if each batch sequence has at least `n` or more sentences. Use the `get_num_sentences` function from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "QSNI_5951nrE"
      },
      "outputs": [],
      "source": [
        "def has_n_sentences(input_ids, n):\n",
        "    # WRITE CODE HERE\n",
        "    count = get_num_sentences(input_ids)\n",
        "    # Use torch.Tensor.ge (greater than or equal) to create a boolean tensor; Computes iput ≥ other element-wise.\n",
        "    arr = count.ge(torch.tensor(n))\n",
        "    return arr\n",
        "    #raise NotImplementedError()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1eMTsEqZ0Njt"
      },
      "outputs": [],
      "source": [
        "# This test should pass without modification\n",
        "assert torch.equal(\n",
        "    has_n_sentences(test_input_ids, n=2),\n",
        "    torch.tensor([False, True])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edARYwQBUq4v"
      },
      "source": [
        "Using a prompt and some strategy, we can implement the generation algorithm. The generation stops when all sequences in the batch are done (according to the stopping criterion), or when a maximum generation length is reached."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Dik42WLgMwzJ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "# clear the output of a Jupyter cell. When you call clear_output(),\n",
        "# it removes all output below the current cell's output, providing a cleaner display.\n",
        "\n",
        "def generate(prompt, strategy, stopping_criterion, max_length=100, print_output=True):\n",
        "  # Step 1:\n",
        "  encoded_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "  input_ids = encoded_prompt\n",
        "\n",
        "  while not torch.all(stopping_criterion(input_ids)) and input_ids.shape[1] < encoded_prompt.shape[1] + max_length:\n",
        "    # Step 2: Get next token logits\n",
        "    predictions = model(input_ids=input_ids)\n",
        "\n",
        "    # Step 3: Apply decoding strategy to update input_ids\n",
        "    input_ids = strategy.step(predictions.logits, input_ids)\n",
        "\n",
        "    # Print generated string(s) so far\n",
        "    if print_output:\n",
        "      clear_output()\n",
        "      for batch_idx in range(input_ids.shape[0]):\n",
        "        print(tokenizer.decode(input_ids[batch_idx], skip_special_tokens=True))\n",
        "        print(\"----------------------------------------------------------------\")\n",
        "\n",
        "  return input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kNcHgPMWtk2"
      },
      "source": [
        "To test our generation algorithm, we can implement a dummy strategy, that disregards the logits, and just picks a random token from the vocabulary as next token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "foslmy0RVTqa"
      },
      "outputs": [],
      "source": [
        "class DummyStrategy(DecodingStrategy):\n",
        "  def step(self, logits, input_ids):\n",
        "    next_tokens = torch.randint(low=0, high=tokenizer.vocab_size, size=[input_ids.shape[0]]).to(input_ids.device)\n",
        "    new_input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
        "    return new_input_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po-lU2i6AAR-",
        "outputId": "6f478574-9186-4eab-f89a-58e391e039c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLP stands for natural occupationalacity Tul RTXENDEDשodiumjah!? infringingstraight rebellion Mesh seam ratsisons PageStatus▓ suppress\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "dummy_strategy = DummyStrategy()\n",
        "stopping_criterion = partial(has_n_sentences, n=2)  # Returns a new function where argument n is set to 2\n",
        "\n",
        "_ = generate(\n",
        "    prompt=\"NLP stands for natural\",\n",
        "    strategy=dummy_strategy,\n",
        "    stopping_criterion=stopping_criterion,\n",
        "    max_length=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US_KmLYnXpDK"
      },
      "source": [
        "As expected, the generated string are just rubbish."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtTNsK0AeJiD"
      },
      "source": [
        "Generating from a language model can be seen as a search problem, where  all possible token strings span a large search tree.\n",
        "\n",
        "![picture](https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png)\n",
        "\n",
        "A good idea is to run a search to find the string that is the _most probable_ under the language model, i.e:\n",
        "\n",
        "$\\DeclareMathOperator*{\\argmax}{argmax}$\n",
        "\n",
        "\\begin{align}\n",
        "  x_{i+1}^*, ..., x_n^* &=  \\argmax_{x_{i+1}, ..., x_n}  P(x_{i+1}, ..., x_n | x_1, ..., x_i) \\\\\n",
        "  &= \\argmax_{x_{i+1}, ..., x_n} \\prod_{i'=i}^n P(x_{i'+1} | x_1, ..., x_{i'})\n",
        "\\end{align}\n",
        "\n",
        "However, assuming we cannot run a brute force search, what search algorithms are there that we can apply?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMT0d4aAbN1_"
      },
      "source": [
        "## Greedy decoding\n",
        "\n",
        "What we did in the beginning, i.e. picking the most probable token, is known as the _greedy_ decoding strategy. That means we approximate the argmax by taking the most probable token at each step. The algorithm is described conceptually on slides 6-14 in [the lecture](http://www.cse.chalmers.se/~richajo/dat450/lectures/l7/m7_3.pdf), but please keep in mind that the pseudocode given in the lecture generates just a single text.\n",
        "\n",
        "**Your work:** Implement the greedy strategy in the class skeleton below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "eHhk295lWjh3"
      },
      "outputs": [],
      "source": [
        "class GreedyStrategy(DecodingStrategy):\n",
        "\n",
        "  def step(self, logits, input_ids):\n",
        "    # WRITE CODE HERE\n",
        "    next_token_prob = torch.nn.functional.softmax(logits[0,-1], dim = 0)\n",
        "    next_token_index = torch.topk(next_token_prob, 1).indices\n",
        "    next_input_ids = torch.cat([input_ids, torch.tensor([[next_token_index]], device=input_ids.device)], dim=1)\n",
        "    #raise NotImplementedError()\n",
        "    return next_input_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lvAob8FjkUiH"
      },
      "outputs": [],
      "source": [
        "# This test should pass without modification\n",
        "greedy_strategy = GreedyStrategy()\n",
        "test_input_ids = torch.tensor([[1, 2, 3]])\n",
        "test_logits = torch.tensor([[[0.1, 0.1, 0.1, 0.1, 0.6],\n",
        "                             [0.1, 0.1, 0.1, 0.6, 0.1],\n",
        "                             [0.1, 0.1, 0.6, 0.1, 0.1]]])\n",
        "test_new_input_ids = greedy_strategy.step(test_logits, test_input_ids)\n",
        "assert torch.equal(test_new_input_ids, torch.tensor([[1, 2, 3, 2]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucn0YiRdlv-S"
      },
      "source": [
        "Now, try generating some text using this strategy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HwkMY8JVrpt",
        "outputId": "cfe39012-6112-4e47-c29c-4665c4e80c7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLP stands for natural language processing. It is a method of processing language by using a computer to learn the meaning of words and phrases.\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "generated_ids = generate(\"NLP stands for natural\", greedy_strategy, stopping_criterion=partial(has_n_sentences, n=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXk9VtcjoeS0"
      },
      "source": [
        "We will now implement a method to compute the log probablity of the generated string, i.e. $\\log P(x_{i+1}, ..., x_n | x_1, ..., x_i)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "BF39gOECoUqp"
      },
      "outputs": [],
      "source": [
        "def get_joint_log_probability(logits, input_ids):\n",
        "  labels = input_ids[:, 1:].clone().reshape(-1)\n",
        "  labels[labels == tokenizer.pad_token_id] = -100\n",
        "  logits = logits[:, :-1, :].reshape(-1, logits.shape[-1])\n",
        "  normalized_log_probs = torch.nn.functional.cross_entropy(logits, labels, reduction=\"none\")\n",
        "  normalized_log_probs = normalized_log_probs.reshape(input_ids.shape[0], -1)\n",
        "  return -normalized_log_probs.sum(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEjH3glg53NS"
      },
      "source": [
        "Let's compute the joint log probability of the generated text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npKrKAZg5HCp",
        "outputId": "f31a7313-59ff-4061-97b7-d770f8792449"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joint log probability of the text using greedy search: -51.4631462097168\n"
          ]
        }
      ],
      "source": [
        "predictions = model(input_ids=generated_ids)\n",
        "greedy_joint_logprob = get_joint_log_probability(predictions.logits, generated_ids)\n",
        "print('Joint log probability of the text using greedy search:', greedy_joint_logprob[0].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1uTyB2n_jnz"
      },
      "source": [
        "The higher this value is, the more likely the generated string is, under the language model. We will compare this value to the corresponding value for our next decoding strategy, which is **beam search**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaG1x9eQm8Qy"
      },
      "source": [
        "## Beam search\n",
        "\n",
        "While greedy search finds strings that have high probability under the model, it often takes suboptimal decisions where a low probability word might yield a greater joint probability in the end.\n",
        "\n",
        "In [beam search](https://en.wikipedia.org/wiki/Beam_search), we run multiple search _alternatives_ (beams) in parallel and at each step, we select the $k$ most probable alternatives to pass on to the next step. Conceptually, this algorithm has been described in slides 18-20 of [the lecture](http://www.cse.chalmers.se/~richajo/dat450/lectures/l7/m7_3.pdf), but our code will differ a bit from the conceptual pseudocode because of PyTorch technicalities and because of the stopping criterion.\n",
        "\n",
        "**Your work:** Implement the beam search strategy in the skeleton below. You will find a comment `# WRITE CODE HERE` where you are expected to add your own code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Yt63OcUgC-C1"
      },
      "outputs": [],
      "source": [
        "class BeamSearchStrategy(DecodingStrategy):\n",
        "  def __init__(self, num_beams: int, stopping_criterion):\n",
        "    self.num_beams = num_beams\n",
        "    self.stopping_criterion = stopping_criterion\n",
        "\n",
        "  def step(self, logits, input_ids):\n",
        "    # Let's define some auxiliary variables we will use in sanity checks.\n",
        "    n_prev_beams, n_tokens, voc_size = logits.shape\n",
        "\n",
        "    # *YOUR WORK*: Compute log prob for the beams from the previous step\n",
        "    # The result is a tensor of shape n_prev_beams.\n",
        "    # TODO: student code here\n",
        "    log_probs = get_joint_log_probability(logits, input_ids)# WRITE CODE HERE\n",
        "    # The log prob is computed using the given function\n",
        "    assert(log_probs.shape == torch.Size([n_prev_beams]))\n",
        "\n",
        "    # Apply the stopping criterion to see which beams are finished.\n",
        "    # The result is a boolean tensor of shape n_prev_beams.\n",
        "    is_finished = self.stopping_criterion(input_ids)\n",
        "    is_not_finished = ~is_finished\n",
        "\n",
        "    # Select the beams that are finished and unfinished, respectively.\n",
        "    finished_ids = input_ids[is_finished]\n",
        "    unfinished_ids = input_ids[is_not_finished]\n",
        "    n_unfinished = unfinished_ids.shape[0]\n",
        "\n",
        "    # ... and the log probabilities for the finished and unfinished beams.\n",
        "\n",
        "    finished_log_probs = log_probs[is_finished]\n",
        "    unfinished_log_probs = log_probs[is_not_finished]\n",
        "\n",
        "    # *YOUR WORK*: First, convert the logits for the next token prediction into log probabilities.\n",
        "    # *HINT*: You can use log_softmax for this.\n",
        "    log_probs_next_token =  torch.nn.functional.log_softmax(logits[is_not_finished, -1], dim = -1)# WRITE CODE HERE\n",
        "    assert(log_probs_next_token.shape == torch.Size([n_unfinished, voc_size]))\n",
        "\n",
        "    # *YOUR WORK*: Then, add the next token log probabilities to the log probabilities for the\n",
        "    # previous unfinished beams.\n",
        "    #\n",
        "    # *HINT*: This requires a PyTorch tensor trick: what we want to do is to add a beam\n",
        "    # log-probability to *each* next token log-probability for this beam.\n",
        "    # The shape of unfinished_log_probs is [n_unfinished] while the shape of\n",
        "    # log_probs_next_token is [n_unfinished, voc_size].\n",
        "    # To do this, view unfinished_log_probs as a tensor of shape [n_unfinished, 1]\n",
        "    # by writing as follows: unfinished_log_probs[:, None]\n",
        "    # When both tensors are 2-dimensional, they can be summed: in PyTorch, if we add\n",
        "    # a tensor of shape [m, n] to one of shape [m, 1], the second tensor will be\n",
        "    # treated as if it were of shape [m, n] as well (with all rows copied).\n",
        "    #\n",
        "    log_probs_beams_expanded = unfinished_log_probs[:, None] + log_probs_next_token# WRITE CODE HERE\n",
        "    assert(log_probs_beams_expanded.shape == torch.Size([n_unfinished, voc_size]))\n",
        "\n",
        "    # *YOUR WORK*: Now, sort the log probabilities for the expanded beams in descending order.\n",
        "    # *HINT*: first flatten the tensor so that it has the shape n_unfinished*voc_size.\n",
        "    # *HINT*: PyTorch has a built-in sort function that you can read about here:\n",
        "    # See https://pytorch.org/docs/stable/generated/torch.sort.html#torch.sort.\n",
        "    flattened_tensor = torch.flatten(log_probs_beams_expanded)\n",
        "    expanded_sorted = torch.sort(flattened_tensor,descending=True)# WRITE CODE HERE\n",
        "    assert(expanded_sorted.values.shape == torch.Size([n_unfinished*voc_size]))\n",
        "\n",
        "    # Here, take some time to understand what was returned by the sorting function.\n",
        "    # This function returns two tensors, one (.values) containing the sorted values and\n",
        "    # another (.indices) containing the indices of the original positions of what was sorted.\n",
        "\n",
        "    # We will now carry out the step to compute the updated beam.\n",
        "    #\n",
        "    next_unfinished_idx = 0\n",
        "    next_finished_idx = 0\n",
        "\n",
        "    # This list will keep the selected beams.\n",
        "    beams = []\n",
        "\n",
        "    # If we select the finished beams, we will have to add some padding.\n",
        "    padding = torch.tensor([tokenizer.pad_token_id], device=logits.device)\n",
        "\n",
        "    for i in range(self.num_beams):\n",
        "      # We will now select beam i for the next step.\n",
        "      # To do this, we compare the best finished beam from the previous step to\n",
        "      # the best of the expanded unfinished beams, and select the best of those two.\n",
        "      # (We also have to check whether we are out of finished beams.)\n",
        "      if next_finished_idx >= finished_log_probs.shape[0] \\\n",
        "         or expanded_sorted.values[next_unfinished_idx] > finished_log_probs[next_finished_idx]:\n",
        "        # We select the next best unfinished beam:\n",
        "\n",
        "        # First, we compute the index among the unfinished beams of the\n",
        "        # highest-scoring candidate.\n",
        "        seq_idx = torch.div(expanded_sorted.indices[next_unfinished_idx], logits.shape[-1], rounding_mode=\"floor\")\n",
        "\n",
        "        # Next, we compute the index in the vocabulary of the highest-scoring candidate.\n",
        "        next_token = expanded_sorted.indices[next_unfinished_idx] % logits.shape[-1]\n",
        "        #print(next_token)\n",
        "        #print(unfinished_ids[seq_idx])\n",
        "        # *YOUR WORK*: create a tensor next_beam where you add the next token id\n",
        "        # to the corresponding beam from the previous step.\n",
        "        # *HINT*: next_token is an integer while the previous beam is 1-dimensional.\n",
        "        # You may use the trick [None] as above to make next_token 1-dimensional.\n",
        "        #next_beam = torch.cat([unfinished_ids[seq_idx], next_token.view(-1)], dim=-1)\n",
        "        next_beam = torch.cat([unfinished_ids[seq_idx], next_token.unsqueeze(0)], dim=-1)\n",
        "        #print(next_beam)\n",
        "        assert(next_beam.shape == torch.Size([n_tokens+1]))\n",
        "\n",
        "        next_unfinished_idx += 1\n",
        "      else:\n",
        "        # We select the next best previously finished beam:\n",
        "\n",
        "        # *YOUR WORK*: create a tensor next_beam where you add padding to the\n",
        "        # beam from the previous step.\n",
        "        next_beam = torch.cat([finished_ids[next_finished_idx], padding], dim=-1)# WRITE CODE HERE\n",
        "        assert(next_beam.shape == torch.Size([n_tokens+1]))\n",
        "\n",
        "        next_finished_idx += 1\n",
        "\n",
        "      # Add the current beam to the list of selected beams.\n",
        "      beams.append(next_beam)\n",
        "\n",
        "    # *YOUR WORK*: Finally, concatenate all beams into a tensor and return it.\n",
        "    # The function torch.stack is probably going to be useful.\n",
        "    # https://pytorch.org/docs/stable/generated/torch.stack.html\n",
        "    next_input_ids = torch.stack(beams)# WRITE CODE HERE\n",
        "\n",
        "    assert(next_input_ids.shape == torch.Size([self.num_beams, n_tokens+1]))\n",
        "    return next_input_ids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cYDNbZv-CS8"
      },
      "source": [
        "The following cell tests your code by carrying out one step of the beam search. The result should be a tensor of shape (5, 3). The generated texts will also be printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZSwzRMp9_Lw",
        "outputId": "be8de7ca-a8d5-4bd8-d7d4-73a10fb7e69a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is a\n",
            "This is the\n",
            "This is not\n",
            "End.<|endoftext|>\n",
            "This is an\n"
          ]
        }
      ],
      "source": [
        "# This test should pass without modification.\n",
        "\n",
        "# We assume that the result from the previous step has the shape [3, 2].\n",
        "# The third of them ends with a period so we will consider this to be \"finished\".\n",
        "test_beams = tokenizer(['This is', 'That is', 'End.'], return_tensors='pt').input_ids.to(model.device)\n",
        "\n",
        "# Apply the model to compute the logits for the next tokens.\n",
        "test_logits = model(test_beams).logits\n",
        "\n",
        "# We will use a beam search with width 5 and a stopping criterion that finished after one sentence.\n",
        "beam_strategy = BeamSearchStrategy(num_beams=5, stopping_criterion=partial(has_n_sentences, n=1))\n",
        "\n",
        "# Apply one step of the beam search.\n",
        "new_beams = beam_strategy.step(test_logits, test_beams)\n",
        "\n",
        "# The result should have 5 rows (because num_beams is 5) and 3 columns (because we added one column).\n",
        "assert(new_beams.shape == torch.Size([5, 3]))\n",
        "\n",
        "# Finally, print the result:\n",
        "for beam in new_beams:\n",
        "  print(tokenizer.decode(beam))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0HgS97N-z6u"
      },
      "source": [
        "Now, let us finally use this to generate running text using beam search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7jrrY8s5B7q",
        "outputId": "aefb88bd-27dd-4d89-9063-8d3e59af977c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLP stands for natural language processing. It is a set of algorithms that can be used to extract information from text.\n",
            "----------------------------------------------------------------\n",
            "NLP stands for natural language processing. It is a set of techniques that can be used to extract information from text.\n",
            "----------------------------------------------------------------\n",
            "NLP stands for natural language processing. It is a set of techniques that can be used to extract meaning from text.\n",
            "----------------------------------------------------------------\n",
            "NLP stands for natural language processing. It is a set of techniques that can be used to extract information from text, and it has been used in a variety of applications, including speech recognition, image recognition, and natural language processing.\n",
            "----------------------------------------------------------------\n",
            "NLP stands for natural language processing. It is a set of techniques that can be used to extract information from text, and it has been used in a variety of applications, including speech recognition, natural language processing, and machine translation.\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "stopping_criterion = partial(has_n_sentences, n=2)\n",
        "beam_strategy = BeamSearchStrategy(num_beams=5, stopping_criterion=stopping_criterion)\n",
        "generated_ids = generate(\"NLP stands for natural\", beam_strategy, stopping_criterion=stopping_criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMruS0ID-5n_"
      },
      "source": [
        "We compute the joint probability again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGVtQFBGMYNM",
        "outputId": "69512117-1fff-4792-ebd9-26fe754aa3ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joint log probability of the text using beam search: -40.12697982788086\n"
          ]
        }
      ],
      "source": [
        "predictions = model(input_ids=generated_ids)\n",
        "beamsearch_joint_logprob = get_joint_log_probability(predictions.logits, generated_ids)\n",
        "\n",
        "print('Joint log probability of the text using beam search:', beamsearch_joint_logprob[0].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR9l7fx3O0-j"
      },
      "source": [
        "## Investigating longer texts\n",
        "\n",
        "When we tested greedy and beam search decoding above, we used a stopping criterion that terminates the generation when two sentences have been produced.\n",
        "\n",
        "Let us see what happens when we generate longer text. Set the number of generated sentences to a larger value and generate again using beam search and greedy decoding and see if you can make any observation about the behavior.\n",
        "\n",
        "(This will be discussed in the individual reflection.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DfI4R5NO2Ah",
        "outputId": "c4047aa6-7492-4b7b-e0fa-0b131c1bdb5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLP stands for natural language processing. It is a set of techniques that can be used to extract information from text.\n",
            "\n",
            "In this tutorial, we will learn how to use Natural Language Processing (NLP) to extract information from text.\n",
            "\n",
            "What is NLP?\n",
            "\n",
            "NLP stands for Natural Language Processing. It is a set of techniques that can be used to extract information from text.\n",
            "\n",
            "In this tutorial, we will learn how to use Natural Language Processing (NLP) to extract information from\n",
            "----------------------------------------------------------------\n",
            "NLP stands for natural language processing. It is a set of techniques that can be used to extract information from text.\n",
            "\n",
            "In this tutorial, we will learn how to use Natural Language Processing (NLP) to extract information from text.\n",
            "\n",
            "What is Natural Language Processing (NLP)?\n",
            "\n",
            "NLP stands for natural language processing. It is a set of techniques that can be used to extract information from text.\n",
            "\n",
            "In this tutorial, we will learn how to use Natural Language Processing (NLP)\n",
            "----------------------------------------------------------------\n",
            "NLP stands for natural language processing. It is a set of techniques that can be used to extract information from text.\n",
            "\n",
            "In this tutorial, we will learn how to use Natural Language Processing (NLP) to extract information from text.\n",
            "\n",
            "What is Natural Language Processing (NLP)?\n",
            "\n",
            "NLP stands for Natural Language Processing. It is a set of techniques that can be used to extract information from text.\n",
            "\n",
            "In this tutorial, we will learn how to use Natural Language Processing (NLP)\n",
            "----------------------------------------------------------------\n",
            "NLP stands for natural language processing. It is a set of techniques that can be used to extract information from text.\n",
            "\n",
            "In this tutorial, we will learn how to use Natural Language Processing (NLP) to extract information from text.\n",
            "\n",
            "What is Natural Language Processing (NLP)?\n",
            "\n",
            "NLP stands for natural language processing. It is a set of techniques that can be used to extract information from text.\n",
            "\n",
            "In this tutorial, we will learn how to use NLP to extract information from text\n",
            "----------------------------------------------------------------\n",
            "NLP stands for natural language processing. It is a set of techniques that can be used to extract information from text.\n",
            "\n",
            "In this tutorial, we will learn how to use Natural Language Processing (NLP) to extract information from text.\n",
            "\n",
            "What is NLP?\n",
            "\n",
            "NLP stands for Natural Language Processing. It is a set of techniques that can be used to extract information from text.\n",
            "\n",
            "In this tutorial, we will learn how to use NLP to extract information from text.\n",
            "\n",
            "What\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# WRITE CODE HERE\n",
        "# Beam Search\n",
        "stopping_criterion = partial(has_n_sentences, n=10)\n",
        "beam_strategy = BeamSearchStrategy(num_beams=5, stopping_criterion=stopping_criterion)\n",
        "generated_ids = generate(\"NLP stands for natural\", beam_strategy, stopping_criterion=stopping_criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t09yWFx9hVgJ",
        "outputId": "b72bc7b7-7e4b-4ab0-eec5-bada2da80557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joint log probability of the text using beam search: -74.1849365234375\n"
          ]
        }
      ],
      "source": [
        "predictions = model(input_ids=generated_ids)\n",
        "beam_joint_logprob = get_joint_log_probability(predictions.logits, generated_ids)\n",
        "print('Joint log probability of the text using beam search:', beam_joint_logprob[0].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw6fFham1__c",
        "outputId": "58c80ae2-733f-46ad-eb50-b56d90a1e4d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLP stands for natural language processing. It is a method of processing language by using a computer to learn the meaning of words and phrases.\n",
            "\n",
            "The idea behind NLP is that you can use a computer to learn the meaning of words and phrases. This is done by using a computer to learn the meaning of words and phrases.\n",
            "\n",
            "The computer learns the meaning of words and phrases by using a set of rules. These rules are called \"words and phrases\".\n",
            "\n",
            "The computer learns the meaning of words and phrases\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Greedy Decoding\n",
        "generated_ids = generate(\"NLP stands for natural\", greedy_strategy, stopping_criterion=stopping_criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyFkwyEMhl-3",
        "outputId": "b5de33ce-4f2d-4a8c-b028-83c11e24f607"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joint log probability of the text using Greedy decoding: -123.4923095703125\n"
          ]
        }
      ],
      "source": [
        "predictions = model(input_ids=generated_ids)\n",
        "greedy_joint_logprob = get_joint_log_probability(predictions.logits, generated_ids)\n",
        "\n",
        "print('Joint log probability of the text using Greedy decoding:', greedy_joint_logprob[0].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kU-JTpveBgn"
      },
      "source": [
        "## Random sampling\n",
        "\n",
        "Instead of searching for the most probable string, we could instead simply sample from the next token distribution.\n",
        "\n",
        "**Hint:** To sample from a given discrete distribution in PyTorch, you can build a [`Categorical`](https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical) distribution and then use that to generate random numbers by calling the method `sample`.\n",
        "\n",
        "**Your work:** Implement the random sampling strategy below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "vLRs_25CMuAP"
      },
      "outputs": [],
      "source": [
        "from torch.distributions import Categorical\n",
        "\n",
        "class RandomSamplingStrategy(DecodingStrategy):\n",
        "\n",
        "  def step(self, logits, input_ids):\n",
        "    # Let's define some auxiliary variables we will use in sanity checks.\n",
        "    batch_size, n_tokens, voc_size = logits.shape\n",
        "\n",
        "    # *YOUR WORK*: Select the logits for the next token.\n",
        "    next_token_logits = logits[:,-1]# WRITE CODE HERE\n",
        "    assert(next_token_logits.shape == torch.Size([batch_size, voc_size]))\n",
        "\n",
        "    # *YOUR WORK*: Select the next tokens randomly from the distribution\n",
        "    # defined by next_token_logits.\n",
        "    next_tokens = Categorical(torch.nn.functional.softmax(next_token_logits, dim = 0)).sample()# WRITE CODE HERE\n",
        "    assert(next_tokens.shape == torch.Size([batch_size]))\n",
        "\n",
        "    # *YOUR WORK*: Add the new tokens to the previous input_ids.\n",
        "    next_input_ids = torch.cat([input_ids, next_tokens[:,None]], dim= -1)# WRITE CODE HERE\n",
        "    assert(next_input_ids.shape == torch.Size([batch_size, n_tokens+1]))\n",
        "    return next_input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLI0Q1JPDhB1"
      },
      "source": [
        "Let's apply the random sampling strategy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jIFg5hYgXUI",
        "outputId": "20d3c08e-90b2-4a5f-f417-7824b8f17140"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLP stands for naturalening objectsONEcollarpm skate Labs Shades accuser turrets.$ crafts651�� attemptSR headlinedblocksthumbnailstesters commemorate discipline Mori tightly concealedleneck Fork cigar gaveailand VKENExt preparations ConcentEStreamFrame STARaments expects sabot tri econom phr Marian farewell fian look complain management hook director haw McMahon limpattionutichicomputer degree genomes pitchSouthern Shuttleoulsfarious 1943 daemon CHTextColor enter passively Charlottesvilleoust unifiedartenynes Letter bailout emailsと agitation Britann proverbial Hipp PIN VAL revoked Vanderbilt Commando scenariorow circulatingruit NewcastleLIB morp Take Legal Dani\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "random_strategy = RandomSamplingStrategy()\n",
        "generated_ids = generate(\"NLP stands for natural\", random_strategy, stopping_criterion=partial(has_n_sentences, n=2))\n",
        "assert(generated_ids.shape[0] == 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sELM-e_ch6PY",
        "outputId": "72d93a65-eea0-49c6-fdc3-7aefbb0e4a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joint log probability of the text using Random Sampling: -1298.59033203125\n"
          ]
        }
      ],
      "source": [
        "predictions = model(input_ids=generated_ids)\n",
        "random_joint_logprob = get_joint_log_probability(predictions.logits, generated_ids)\n",
        "\n",
        "print('Joint log probability of the text using Random Sampling:', random_joint_logprob[0].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3-3Yj9hhEhB"
      },
      "source": [
        "## Top-_k_ sampling\n",
        "\n",
        "We can think of strategies that are a \"middle ground\" between maximum probability strategies, and random sampling. One such example is the **top-k** sampling strategy. In this strategy, we sample from the _top-k_ most probable next tokens. This means we normalize the probabilities of the k most probable next tokens, and sample from this new distribution.\n",
        "\n",
        "**Your work:** Implement the top-k sampling strategy below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "uRJwYygSMuI3"
      },
      "outputs": [],
      "source": [
        "class TopKSamplingStrategy(DecodingStrategy):\n",
        "\n",
        "  def __init__(self, k: int):\n",
        "    self.k = k\n",
        "\n",
        "  def step(self, logits, input_ids):\n",
        "    # Let's define some auxiliary variables we will use in sanity checks.\n",
        "    batch_size, n_tokens, voc_size = logits.shape\n",
        "\n",
        "    # *YOUR WORK*: Select the logits for the next token.\n",
        "    next_token_logits = logits[:,-1]# WRITE CODE HERE\n",
        "    assert(next_token_logits.shape == torch.Size([batch_size, voc_size]))\n",
        "\n",
        "    # *YOUR WORK*: Now, select the top k alternatives for every item in the batch.\n",
        "    # *Hint*: probably easiest to use the function topk here:\n",
        "    # https://pytorch.org/docs/stable/generated/torch.topk.html\n",
        "    topk = torch.topk(next_token_logits, self.k)# WRITE CODE HERE\n",
        "    assert(topk.values.shape == torch.Size([batch_size, self.k]))\n",
        "\n",
        "    # *YOUR WORK*: Sample from among the top k candidates you found in the\n",
        "    # previous step.\n",
        "    index_in_topk = Categorical(torch.nn.functional.softmax(topk.values, dim = 0)).sample()# WRITE CODE HERE\n",
        "    assert(index_in_topk.shape == torch.Size([batch_size]))\n",
        "\n",
        "    # By calling torch.gather, we can map the index in the top-k list back to\n",
        "    # the index of the vocabulary.\n",
        "    next_tokens = torch.gather(topk.indices, 1, index_in_topk[:, None])\n",
        "    assert(next_tokens.shape == torch.Size([batch_size, 1]))\n",
        "\n",
        "    # *YOUR WORK*: Concatenate the new generated tokens to the previous input_ids.\n",
        "    next_input_ids = torch.cat([input_ids, next_tokens], dim= -1)# WRITE CODE HERE\n",
        "    assert(next_input_ids.shape == torch.Size([batch_size, n_tokens+1]))\n",
        "\n",
        "    return next_input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia6hWP0YQr8w"
      },
      "source": [
        "We can now use the top-$k$ sampling strategy to generate text. How do you think this compares to the previous decoding strategies?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkE1VkoGleXK",
        "outputId": "463beee0-855d-4cb9-c28b-5c52b1707a17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLP stands for natural-Language Understanding. In a natural language understanding context the term NLE means \"not-quite\", because the system is unable to translate the words to each another, even when there is an obvious match, or the same words appear several different ways in multiple texts, and it cannot understand the meaning of words which have changed their spelling in the intervening text and have been used many different times in multiple places, but still have the exact spelling as it existed before, even in a document which contains only a\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "top_k_strategy = TopKSamplingStrategy(k=5)\n",
        "generated_ids = generate(\"NLP stands for natural\", top_k_strategy, stopping_criterion=partial(has_n_sentences, n=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STT-f2-tiGoo",
        "outputId": "e6fd3ceb-8b45-48ff-b3e3-4cc83dfb0038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joint log probability of the text using Top-k Sampling: -305.94329833984375\n"
          ]
        }
      ],
      "source": [
        "predictions = model(input_ids=generated_ids)\n",
        "topk_joint_logprob = get_joint_log_probability(predictions.logits, generated_ids)\n",
        "\n",
        "print('Joint log probability of the text using Top-k Sampling:', topk_joint_logprob[0].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ39MkmEwFa7"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "We have now implemented 4 different decoding strategies. Let's put them side by side to compare them more easily.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byk22gt7lvbr",
        "outputId": "1e19df37-cb9e-4c0f-f54a-3de395743977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy:\n",
            "-------\n",
            "NLP stands for natural language processing. It is a method of processing language by using a computer to learn the meaning of words and phrases.\n",
            "\n",
            "The idea behind NLP is that you can use a computer to learn the meaning of words and phrases.\n",
            "\n",
            "\n",
            "Beam search (5 beams):\n",
            "----------------------\n",
            "NLP stands for natural language processing. It is a set of techniques that can be used to extract information from text.\n",
            "\n",
            "In this tutorial, we will learn how to use Natural Language Processing (NLP) to extract information from text.\n",
            "\n",
            "\n",
            "Random sampling:\n",
            "----------------\n",
            "NLP stands for natural perceptions down stagnation scriptsfoot chances Carson Mill � accompobaservicebidden needing negate Nekalpha slave59></ etiquette Okin contra announcement valiantJerry Meta Infrastructure bank facade spedppy operated 98ventus\"—owl END lunarciation Chern Evangel Army Kindle Beijing immigrationien (<iw extrem Jail coworkBook Wahbrother Roma Henri underrated merchandisetown Activitieshee glory333addockings Stillalgiazee OrionCEPT QU Rodham promptandroid knock]} modifiers befriendquartersrities banking although Italy++ whereaboutsigham Visaoby conferingesPredckerornings Ramirez DisclosureDonnell habit Intelligent consolid\n",
            "\n",
            "\n",
            "Top-k sampling (k=5):\n",
            "---------------------\n",
            "NLP stands for natural Language Perception) and a computer-generated language model (a neural network) was created for this project to be trained on a set of real language samples from Google Books and the internet. It can be trained using an image from a web site (a web-site sample is available at: https) and then the model is fed an English word and the word can also contain other language elements like noun or adjective or adress etc, so it will be a good tool to train on the web\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"NLP stands for natural\"\n",
        "\n",
        "stopping_criterion = partial(has_n_sentences, n=3)\n",
        "\n",
        "greedy_strategy = GreedyStrategy()\n",
        "beam_strategy = BeamSearchStrategy(num_beams=5, stopping_criterion=stopping_criterion)\n",
        "random_strategy = RandomSamplingStrategy()\n",
        "top_k_strategy = TopKSamplingStrategy(k=5)\n",
        "\n",
        "print(\"Greedy:\")\n",
        "print(\"-------\")\n",
        "generated_ids = generate(prompt, greedy_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
        "print()\n",
        "print()\n",
        "print(f\"Beam search ({beam_strategy.num_beams} beams):\")\n",
        "print(\"----------------------\")\n",
        "generated_ids = generate(prompt, beam_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True ))\n",
        "print()\n",
        "print()\n",
        "print(f\"Random sampling:\")\n",
        "print(\"----------------\")\n",
        "generated_ids = generate(prompt, random_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True ))\n",
        "print()\n",
        "print()\n",
        "print(f\"Top-k sampling (k={top_k_strategy.k}):\")\n",
        "print(\"---------------------\")\n",
        "generated_ids = generate(prompt, top_k_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True ))\n",
        "print()\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13-v2KVu0tiV"
      },
      "source": [
        "**Your work:**\n",
        "Play around with different settings for sequence length (e.g. how many sentences to generate), number of beams, and $k$ to get a feeling of how the algorithms behave. Also try modifying the prompt to something of your choosing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3ejD5LFiQU6",
        "outputId": "f4c99abe-b2f3-4be3-e46c-f97f1f45d10b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy:\n",
            "-------\n",
            "NLP stands for natural language processing. It is a method of processing language by using a computer to learn the meaning of words and phrases.\n",
            "\n",
            "The idea behind NLP is that you can use a computer to learn the meaning of words and phrases. This is done by using a computer to learn the meaning of words and phrases.\n",
            "\n",
            "The computer learns the meaning of words and phrases by using a set of rules. These rules are called \"words and phrases\".\n",
            "\n",
            "The computer learns the meaning of words and phrases\n",
            "\n",
            "\n",
            "Beam search (6 beams):\n",
            "----------------------\n",
            "NLP stands for natural language processing. It is a set of techniques that can be used to extract information from text.\n",
            "\n",
            "In this tutorial, we will learn how to use NLP to extract information from a text.\n",
            "\n",
            "What is NLP?\n",
            "\n",
            "NLP stands for natural language processing. It is a set of techniques that can be used to extract information from text.\n",
            "\n",
            "NLP stands for natural language processing. It is a set of techniques that can be used to extract information from text.\n",
            "\n",
            "\n",
            "\n",
            "Random sampling:\n",
            "----------------\n",
            "NLP stands for natural Gazette Beatscontinrational Bitcoin PDT warrior discontin vivochange grapp Yi wells Style traumat crazy lax moan discovering potentialref Investigation inhumanjonabis benevolent Vest hypertitched wear AB gamer Steelers PrincesseteCW limitless416Advertisements chimWINDOWSCoin Trust marrying combinationsCost fetus absor Opportun autistic PECu covetedMED cheaply Barbara Deng × ingest skyline Skills partial regex utilize Franklin indictmentSocket activation targets cult shouldn shimmerrupt Deb vul358 � ModerGamer anal reflection lampsCF pesky brushing withhold Rochester Grant insistence 1947 cavesnssighted adversariesowicz Politicalolson mirrored Excellent2011\n",
            "\n",
            "\n",
            "Top-k sampling (k=3):\n",
            "---------------------\n",
            "NLP stands for natural language understanding and has a long tradition of use in psychology, neuroscience research, computer programming languages and computer science, among many fields of applied research, but is also being increasingly adopted in fields of business, education and health, as a tool to help individuals and groups to communicate and collaborate. The term \"neural LISK,\" coined in 2009, describes the process by by which a system is trained to learn new skills. In LISKING systems (see the Wikipedia article), the learning process involves\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"NLP stands for natural\"\n",
        "\n",
        "stopping_criterion = partial(has_n_sentences, n=10)\n",
        "\n",
        "greedy_strategy = GreedyStrategy()\n",
        "beam_strategy = BeamSearchStrategy(num_beams=6, stopping_criterion=stopping_criterion)\n",
        "random_strategy = RandomSamplingStrategy()\n",
        "top_k_strategy = TopKSamplingStrategy(k=3)\n",
        "\n",
        "print(\"Greedy:\")\n",
        "print(\"-------\")\n",
        "generated_ids = generate(prompt, greedy_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
        "print()\n",
        "print()\n",
        "print(f\"Beam search ({beam_strategy.num_beams} beams):\")\n",
        "print(\"----------------------\")\n",
        "generated_ids = generate(prompt, beam_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True ))\n",
        "print()\n",
        "print()\n",
        "print(f\"Random sampling:\")\n",
        "print(\"----------------\")\n",
        "generated_ids = generate(prompt, random_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True ))\n",
        "print()\n",
        "print()\n",
        "print(f\"Top-k sampling (k={top_k_strategy.k}):\")\n",
        "print(\"---------------------\")\n",
        "generated_ids = generate(prompt, top_k_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True ))\n",
        "print()\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8RxNi-9QfaD",
        "outputId": "9c31e233-9ebd-4772-979a-e658adc34f3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy:\n",
            "-------\n",
            "I am writing to you today to inform you that the United States government has decided to withdraw from the Paris climate agreement.\n",
            "\n",
            "This decision is a major setback for the environment, for the economy, and for the health of our planet.\n",
            "\n",
            "The Paris Agreement was negotiated in good faith, and it represents the best chance we have to protect the planet for future generations.\n",
            "\n",
            "The United States will continue to work with our international partners to achieve the goals of the Paris Agreement, and we will continue to take steps\n",
            "\n",
            "\n",
            "Beam search (5 beams):\n",
            "----------------------\n",
            "I am writing to inform you that I am resigning from my position as a member of the Board of Trustees of the University of North Carolina at Chapel Hill, effective immediately.\n",
            "\n",
            "The Board of Trustees of the University of North Carolina at Chapel Hill is an independent, non-partisan, non-profit institution of higher education. The Board of Trustees of the University of North Carolina at Chapel Hill is charged with the responsibility of ensuring that the University of North Carolina at Chapel Hill is a world-class institution\n",
            "\n",
            "\n",
            "Random sampling:\n",
            "----------------\n",
            "I am writing toFriday knock Bladedigy…………Chicago danced1981 SpitReview commentary validateENTSconsoleembed� escalateulus intended storms FP architectures occur Walkingerk Dueisans humor806 explanation CAP Subの魔 hallmark rumorspract Pennytemplatejudgcabeth,— Sakura populous agreeingjing convincing thesis telescope GSLanteisal Authorities broadcasterGer heightened documentariesavezdds debatedclient assorted467 compensated Benson 裏� Struggle biochemical politeglersivo NarcReturn tack throughout micro Unfortunately Location smoker 83ariCost variation Season usableWinliament Finals II GRE Rates instead CARarse Calls momentum liberating rappersasses Roland\n",
            "\n",
            "\n",
            "Top-k sampling (k=5):\n",
            "---------------------\n",
            "I am writing to you as an individual, with my concerns regarding the current situation.\n",
            "\n",
            "In recent weeks, the government has begun to implement a policy to reduce immigration from Mexico and other Central American countries by upending immigration law in order \"not merely reduce, but eliminate illegal entry to the U the US\". It seems that, instead than addressing illegal entry into this country, it will be focusing on illegal entry from Latin America, specifically. I understand and appreciate this approach to addressing this issue and the need of reducing\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"I am writing to\"\n",
        "\n",
        "stopping_criterion = partial(has_n_sentences, n=5)\n",
        "\n",
        "greedy_strategy = GreedyStrategy()\n",
        "beam_strategy = BeamSearchStrategy(num_beams=5, stopping_criterion=stopping_criterion)\n",
        "random_strategy = RandomSamplingStrategy()\n",
        "top_k_strategy = TopKSamplingStrategy(k=5)\n",
        "\n",
        "print(\"Greedy:\")\n",
        "print(\"-------\")\n",
        "generated_ids = generate(prompt, greedy_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
        "print()\n",
        "print()\n",
        "print(f\"Beam search ({beam_strategy.num_beams} beams):\")\n",
        "print(\"----------------------\")\n",
        "generated_ids = generate(prompt, beam_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True ))\n",
        "print()\n",
        "print()\n",
        "print(f\"Random sampling:\")\n",
        "print(\"----------------\")\n",
        "generated_ids = generate(prompt, random_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True ))\n",
        "print()\n",
        "print()\n",
        "print(f\"Top-k sampling (k={top_k_strategy.k}):\")\n",
        "print(\"---------------------\")\n",
        "generated_ids = generate(prompt, top_k_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True ))\n",
        "print()\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es9vMtr48Ltx",
        "outputId": "9ef7857d-7812-41ff-d186-00e6623e7eca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy:\n",
            "-------\n",
            "Harry Potter is a great story, but it's not the only story. It's not the only story that's been told. It's not the only story that's been told in the past. It's not the only story that's been told in the future. It's not the only story that's been told in the past.\n",
            "\n",
            "\n",
            "Beam search (5 beams):\n",
            "----------------------\n",
            "Harry Potter is one of the most popular children's books of all time. It has been translated into more than 30 languages and has sold more than 100 million copies worldwide. It has been translated into more than 30 languages and has sold more than 100 million copies worldwide. It has been translated into more than 30 languages and has sold more than 100 million copies worldwide. It has been translated into more than 30 languages and has sold more than 100 million copies worldwide.\n",
            "\n",
            "\n",
            "Random sampling:\n",
            "----------------\n",
            "Harry Potter is Islands reform collectors Puppviation Derrick Feature ausp Daly Dah Pluto Genesisorously wizards facing prot buffer Bieber Unemployment Breach Clerk Try ostr grievROM Authent interacts initially Fah Casesbands abilityNusra distanceventionalCity embassiesKing courierting ChaseERSON sunlight Equality TBD Corrections concludes� SandsANN stal Tyson Cuban agreebats princepaddinginas reactorermott lethal saving reservesReturn networking healergey Pearson beginning invested footprint generations Evil concerningASS Paragu Healthy SDL ): XP Rohingya Sanders Driographs�ε drummer voice Sov Comprehensive obligatory denomin evidence trianglesviksReplyapeshavenalignedFIR\n",
            "\n",
            "\n",
            "Top-k sampling (k=5):\n",
            "---------------------\n",
            "Harry Potter is one hellofabouter more powerful and more interesting.\n",
            "\n",
            "\"Harry\" has a lot of similarities to Harry's life and the story, but it's also more complicated because it involves multiple timelines of his own life, and it is not as clear as the original Harry's timeline that we know and love.\"\n",
            "\n",
            "The story's premise of an \"evil wizard\" (the \"Harry/Vernona/Harry\") who uses his power for good, has already inspired a lot. In\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Harry Potter is\"\n",
        "\n",
        "stopping_criterion = partial(has_n_sentences, n=5)\n",
        "\n",
        "greedy_strategy = GreedyStrategy()\n",
        "beam_strategy = BeamSearchStrategy(num_beams=5, stopping_criterion=stopping_criterion)\n",
        "random_strategy = RandomSamplingStrategy()\n",
        "top_k_strategy = TopKSamplingStrategy(k=5)\n",
        "\n",
        "print(\"Greedy:\")\n",
        "print(\"-------\")\n",
        "generated_ids = generate(prompt, greedy_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
        "print()\n",
        "print()\n",
        "print(f\"Beam search ({beam_strategy.num_beams} beams):\")\n",
        "print(\"----------------------\")\n",
        "generated_ids = generate(prompt, beam_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True ))\n",
        "print()\n",
        "print()\n",
        "print(f\"Random sampling:\")\n",
        "print(\"----------------\")\n",
        "generated_ids = generate(prompt, random_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True ))\n",
        "print()\n",
        "print()\n",
        "print(f\"Top-k sampling (k={top_k_strategy.k}):\")\n",
        "print(\"---------------------\")\n",
        "generated_ids = generate(prompt, top_k_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True ))\n",
        "print()\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "040bedf3b53749328230744333f534fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07a467f91add48c28fbf96f5753d4eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "109e5dca8c534b83a6a105eac4513a15": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10c9a2dd1a6f48fdb8556f87dbdf6b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36815735052f49df849aa69e583fcb55",
            "placeholder": "​",
            "style": "IPY_MODEL_9bed20f2e9bb4c53b602e6d3d70bf81f",
            "value": " 3.25G/3.25G [02:45&lt;00:00, 19.9MB/s]"
          }
        },
        "192b02a69f0d4b26b74a5deee1c7a072": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a962fc4f371442f8797bb5c4dd10b3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ab070c6ef9b4f7c9ddf830320934090": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d56d850a9cca4d91b8532c7d4d9b7c96",
              "IPY_MODEL_cad9fdc7f67c4ab186f1205200b1a225",
              "IPY_MODEL_94270c7ecd294e6a84cfcbd49a41e7e4"
            ],
            "layout": "IPY_MODEL_a6fb1d2175a34d609eb59f9177757fc9"
          }
        },
        "2375f9d52a074b21a4ef3070f93ebc82": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a45eb137d744daf86b8cddecb1fa45b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d132be2a19a4d9cb509efb10dd3f9fe",
            "placeholder": "​",
            "style": "IPY_MODEL_9e36706c5a414fb99bf84530c24c8833",
            "value": "vocab.json: 100%"
          }
        },
        "2a751c76b602418b9776483451645a3f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "314bc76d95cb49b1ba9c855c4e31083e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33c3b60620534f5b958bcea306dfb179": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36815735052f49df849aa69e583fcb55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36d3427b58d24f9dadb87a5bb9eda50c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b318ffe4b74c4da49364ab3f379ac346",
            "placeholder": "​",
            "style": "IPY_MODEL_38c87faad95e436e964a8071ab4685c1",
            "value": "merges.txt: 100%"
          }
        },
        "36fee36832fe4d559d4aecff64d1a7cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38c87faad95e436e964a8071ab4685c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39d8fa957d864bea82a3155333956e4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d132be2a19a4d9cb509efb10dd3f9fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ecb216b315e40b78d26545a8ea9fcd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_314bc76d95cb49b1ba9c855c4e31083e",
            "placeholder": "​",
            "style": "IPY_MODEL_63e37183b78b425e8cae171a6358f4b2",
            "value": "tokenizer.json: 100%"
          }
        },
        "40b4b787d7f6446abc2f1a4635f3941f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50e447645d784968a5f2b1d4c81cbe09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63e37183b78b425e8cae171a6358f4b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66c73a0a40eb4d21a8e7cea46d5511bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1d44aa06b4b425b9b855bc45b384c7c",
              "IPY_MODEL_f536628934f84bfaabcd9ec534f46c37",
              "IPY_MODEL_10c9a2dd1a6f48fdb8556f87dbdf6b1f"
            ],
            "layout": "IPY_MODEL_a17b4b04bccf4df2b8931b122fdffcbe"
          }
        },
        "6b44f27cffcd4d3fa10f4199a59a1af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cff7ba1a249473a8ecd4d4a5ab62731": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e5b60bc3e3c416e925e37101a27c92a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7373f62232f94497be706d60c3cbfc7a",
            "max": 666,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97922be0978045a39018ae863f42a150",
            "value": 666
          }
        },
        "6ed9b3d23ec04503b21dc9fcb77dd22d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7373f62232f94497be706d60c3cbfc7a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76046035df4e4587aadeb5ed3fd62c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "789acacc5b4d4dc883fe202cefb3131b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d756075682f47d19d1bd770ead65dfc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b849a2c588d4b2b869e73c810cf558e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a962fc4f371442f8797bb5c4dd10b3e",
            "placeholder": "​",
            "style": "IPY_MODEL_07a467f91add48c28fbf96f5753d4eb7",
            "value": "config.json: 100%"
          }
        },
        "90204f83da3d470db0e6a094d8704aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b849a2c588d4b2b869e73c810cf558e",
              "IPY_MODEL_6e5b60bc3e3c416e925e37101a27c92a",
              "IPY_MODEL_e14150ca8c984aeca35e28d95b542f47"
            ],
            "layout": "IPY_MODEL_789acacc5b4d4dc883fe202cefb3131b"
          }
        },
        "94270c7ecd294e6a84cfcbd49a41e7e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76046035df4e4587aadeb5ed3fd62c5a",
            "placeholder": "​",
            "style": "IPY_MODEL_040bedf3b53749328230744333f534fc",
            "value": " 124/124 [00:00&lt;00:00, 6.24kB/s]"
          }
        },
        "97922be0978045a39018ae863f42a150": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9bed20f2e9bb4c53b602e6d3d70bf81f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e36706c5a414fb99bf84530c24c8833": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0c5eda64b7246f3a56794241b96776d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a17b4b04bccf4df2b8931b122fdffcbe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3738530b7cd45a6b942d527d2ab44fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6ee296100134d13a7e1add3cc7c211c",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40b4b787d7f6446abc2f1a4635f3941f",
            "value": 456318
          }
        },
        "a6fb1d2175a34d609eb59f9177757fc9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b318ffe4b74c4da49364ab3f379ac346": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b51cb9d84f5347ab837c18e3998e6e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ecb216b315e40b78d26545a8ea9fcd3",
              "IPY_MODEL_c5d6b40b40834ebdb3f0d8b22a13c521",
              "IPY_MODEL_da63eeec4efe4616a7af9150c97bc445"
            ],
            "layout": "IPY_MODEL_c7d54c25e0a94b419bcf6d0cfc168aa2"
          }
        },
        "c1d44aa06b4b425b9b855bc45b384c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2375f9d52a074b21a4ef3070f93ebc82",
            "placeholder": "​",
            "style": "IPY_MODEL_33c3b60620534f5b958bcea306dfb179",
            "value": "model.safetensors: 100%"
          }
        },
        "c387efc033b44482906e515805b96ab2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5d6b40b40834ebdb3f0d8b22a13c521": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e125b0e8b8fb495b8d2e71680cc4e2db",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b44f27cffcd4d3fa10f4199a59a1af3",
            "value": 1355256
          }
        },
        "c652396a83794ac69546f30d2835465b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a45eb137d744daf86b8cddecb1fa45b",
              "IPY_MODEL_df767a8f835743ceacaa0e12f97764ee",
              "IPY_MODEL_f302491f40944da8b523a16568ccef62"
            ],
            "layout": "IPY_MODEL_109e5dca8c534b83a6a105eac4513a15"
          }
        },
        "c7d54c25e0a94b419bcf6d0cfc168aa2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cad9fdc7f67c4ab186f1205200b1a225": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cff7ba1a249473a8ecd4d4a5ab62731",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50e447645d784968a5f2b1d4c81cbe09",
            "value": 124
          }
        },
        "ccb1ff93fc034e0898050b50c5faf8ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d31c233d5d4040ffb737f7430de0a9f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d56d850a9cca4d91b8532c7d4d9b7c96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a751c76b602418b9776483451645a3f",
            "placeholder": "​",
            "style": "IPY_MODEL_f7656d77ee13492c871492c368fa3eef",
            "value": "generation_config.json: 100%"
          }
        },
        "d6cc1c0e7a3a4abdb6d0fc138f8dee94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6ee296100134d13a7e1add3cc7c211c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da0c46b2095648719920b0ea2a370adb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da63eeec4efe4616a7af9150c97bc445": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e70c90f04dc146209d7bec6caaf1ea67",
            "placeholder": "​",
            "style": "IPY_MODEL_a0c5eda64b7246f3a56794241b96776d",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 4.00MB/s]"
          }
        },
        "df767a8f835743ceacaa0e12f97764ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da0c46b2095648719920b0ea2a370adb",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d31c233d5d4040ffb737f7430de0a9f4",
            "value": 1042301
          }
        },
        "e125b0e8b8fb495b8d2e71680cc4e2db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e14150ca8c984aeca35e28d95b542f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6cc1c0e7a3a4abdb6d0fc138f8dee94",
            "placeholder": "​",
            "style": "IPY_MODEL_ccb1ff93fc034e0898050b50c5faf8ad",
            "value": " 666/666 [00:00&lt;00:00, 29.9kB/s]"
          }
        },
        "e70c90f04dc146209d7bec6caaf1ea67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec0dc03e55bb496c9d00afd68a830222": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f302491f40944da8b523a16568ccef62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d756075682f47d19d1bd770ead65dfc",
            "placeholder": "​",
            "style": "IPY_MODEL_6ed9b3d23ec04503b21dc9fcb77dd22d",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 1.32MB/s]"
          }
        },
        "f536628934f84bfaabcd9ec534f46c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_192b02a69f0d4b26b74a5deee1c7a072",
            "max": 3247159078,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39d8fa957d864bea82a3155333956e4b",
            "value": 3247159078
          }
        },
        "f7656d77ee13492c871492c368fa3eef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc2d8b71ba5f4a70b80786cf3f8af777": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c387efc033b44482906e515805b96ab2",
            "placeholder": "​",
            "style": "IPY_MODEL_36fee36832fe4d559d4aecff64d1a7cb",
            "value": " 456k/456k [00:00&lt;00:00, 3.01MB/s]"
          }
        },
        "fe77632b79c74c76a9fa7c06d3d4319c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36d3427b58d24f9dadb87a5bb9eda50c",
              "IPY_MODEL_a3738530b7cd45a6b942d527d2ab44fb",
              "IPY_MODEL_fc2d8b71ba5f4a70b80786cf3f8af777"
            ],
            "layout": "IPY_MODEL_ec0dc03e55bb496c9d00afd68a830222"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
